# Stock Price Prediction Using Multiple Algorithms
##Introduction:


*   Machine learning and deep learning have found their place in the financial institutions for their power in predicting time series data with high degrees of accuracy and the research is still going on to make the models better. This project was done in order to complete the academic project of Data Mining.

##Project workflow
The workflow for this project is essentially in these steps:
*  Problem Understanding
*  Linear Regression
*  KNN
*  LSTM (Deep Learning)
*  Conclusion



#1- Problem Understanding
###What are Stocks?
*  Stock represents a claim on the company's assets and earnings. As you acquire more stock, your ownership stake in the company becomes greater.” 

###What is stock price prediction? 

*  Stock market prediction is the act of trying to determine the future value of a company stock or other financial instrument traded on an exchange. The successful prediction of a stock's future price could yield significant profit. 
*  Stock market analysis is divided into two parts:
      1.   Fundamental analysis : involves analyzing a company’s financial statements to determine the fair value of the business 
      2.   Technical analysis : assumes that a security’s price already reflects all publicly-available information and instead focuses on the statistical analysis of price movements.


We will use dataset from Quandl (A website where you can find historical data of stocks) mainly the (TATAGLOBAL) stock.



###Time to dive in !!
Let's load dataset and packages ! 
#import packages
import pandas as pd
import numpy as np

#to plot within notebook
import matplotlib.pyplot as plt
%matplotlib inline

#setting figure size
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 20,10

#for normalizing data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

#read the file
!pip install quandl
import quandl
df = quandl.get("NSE/TATAGLOBAL")

#print the head
df.head()

###We notice that there are many variables in our dataset :


*   Open & Close : represents the starting and the final price of the stock in a day.
*   Low & High & Last : represent the Low , High prices of the stock during the day & the last price of the share of the day.
*  Total Trade Quantity : represent the number of shares sold.
*  Turnover (Lacs) : represent the turnover of the company.

Let's now plot the y variable (Close) to see how is the shape of it.
#plot
plt.figure(figsize=(16,8))
plt.plot(df['Close'], label='Close Price history')
#Linear Regression

###Introduction : 
Let's start by the most basic algorithm that can be implimented on this usecase.

*  Linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). 


###Implementation :

We will first sort the dataset in ascending order and then create a separate dataset so that any new feature created does not affect the original data.


#setting index as date values
df['Date'] = pd.to_datetime(df.index.values,format='%Y-%m-%d')
df.index = df['Date']

#sorting
data = df.sort_index(ascending=True, axis=0)

#creating a separate dataset
new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])

for i in range(0,len(data)):
    new_data['Date'][i] = data['Date'][i]
    new_data['Close'][i] = data['Close'][i]

new_data.head()
!pip install fastai==0.7.0

from fastai.structured import  add_datepart
add_datepart(new_data, 'Date')
new_data.drop('Elapsed', axis=1, inplace=True)  #elapsed will be the time stamp
new_data.head()
*  The add_datepart creates features based on the date variable.

*   I guess that the first and last day of the week affect the changement of stock's price more than normal days do!

*  So let's create a new column which will contain the value 1 if its Monday or Friday and the value of 0 if not.
new_data['mon_fri'] = 0
for i in range(0,len(new_data)):
    if (new_data['Dayofweek'][i] == 0 or new_data['Dayofweek'][i] == 4):
        new_data['mon_fri'][i] = 1
    else:
        new_data['mon_fri'][i] = 0
new_data.head()
If the day of the week is equal to 0 or 4 , the mon_fri variable will set to 1 (True) other wise it will be set as a 0.

We will now split the data into train and test set and then build our model.

*   The Train set will be for training our model.
*   The Test set will be for validation our model & Calculating its accuracy.
#split into train and validation
train = new_data[:987]
valid = new_data[987:]

x_train = train.drop('Close', axis=1)
y_train = train['Close']
x_valid = valid.drop('Close', axis=1)
y_valid = valid['Close']

#implement linear regression
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train,y_train)
###Results

Let's now calculate the RMSE to check the accuracy of our model
#make predictions and find the rmse
preds = model.predict(x_valid)
rms=np.sqrt(np.mean(np.power((np.array(y_valid)-np.array(preds)),2)))
rms
The RMSE (root mean square error) value is high, which clearly shows that linear regression has performed poorly and we can not count on this model. 
###Let’s look at the plot and understand why linear regression has not done well:

#plot
valid['Predictions'] = 0
valid['Predictions'] = preds

valid.index = new_data[987:].index
train.index = new_data[:987].index

plt.plot(train['Close'])
plt.plot(valid[['Close', 'Predictions']])
###Conclusion
In fact, Linear Regression is a simple algorithm that can be implemented quickly, But it has a few cons, which are affecting forecasting problems, since the model overfits the date column and not taking in consideration the last predicted point.
#k-Nearest Neighbours Regression

###Introduction : 
Let's now dive in with another known algorithm 
*  the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression In both cases, the input consists of the k closest training examples in the feature space. 


###Implementation :

We will use the same dataset (train and validation)
#importing libraries
from sklearn import neighbors
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
#scaling data
x_train_scaled = scaler.fit_transform(x_train)
x_train = pd.DataFrame(x_train_scaled)
x_valid_scaled = scaler.fit_transform(x_valid)
x_valid = pd.DataFrame(x_valid_scaled)

#using gridsearch to find the best parameter
params = {'n_neighbors':[2,3,4,5,6,7,8,9]}
knn = neighbors.KNeighborsRegressor()
model = GridSearchCV(knn, params, cv=5)

#fit the model and make predictions
model.fit(x_train,y_train)
preds = model.predict(x_valid)
###Result
#rmse
rms=np.sqrt(np.mean(np.power((np.array(y_valid)-np.array(preds)),2)))
rms
There is not a huge difference in the RMSE value, but a plot for the predicted and actual values should provide a more clear understanding.


#plot
valid['Predictions'] = 0
valid['Predictions'] = preds
plt.plot(valid[['Close', 'Predictions']])
plt.plot(train['Close'])
###Conclusion
The KNN regression algorithm got us a close RMSE to the linear regression,
We can say that regression algorithms have not performed well on this dataset
#Long Short Term Memory (LSTM) - RNN

###Introduction : 
As long as the regular machine learning algorithms doesnt go well with our data set, it means that we should change the direction to the aka DEEP LEARNING.
We are having a timeseries based data set, so the first approach that comes in mind is RNN, particulary LSTM, because the model will be based on both past and present info.

*  Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. 
*  LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!

LSTM has three gates:



1.   The input gate: The input gate adds information to the cell state
2.   The forget gate: It removes the information that is no longer required by the model
3.   The output gate: Output Gate at LSTM selects the information to be shown as output








###Implementation :

For now, let us implement LSTM and check it’s performance on our particular data.


#importing required libraries
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM

#creating dataframe
data = df.sort_index(ascending=True, axis=0)
new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])
for i in range(0,len(data)):
    new_data['Date'][i] = data['Date'][i]
    new_data['Close'][i] = data['Close'][i]

#setting index
new_data.index = new_data.Date
new_data.drop('Date', axis=1, inplace=True)

#creating train and test sets
dataset = new_data.values

train = dataset[0:987,:]
valid = dataset[987:,:]

#converting dataset into x_train and y_train
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)

x_train, y_train = [], []
for i in range(60,len(train)):
    x_train.append(scaled_data[i-60:i,0])
    y_train.append(scaled_data[i,0])
x_train, y_train = np.array(x_train), np.array(y_train)

x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))


Now we the data is well prepared, Let's create the model
# create and fit the LSTM network
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))
model.add(LSTM(units=50))
model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)

#predicting 246 values, using past 60 from the train data
inputs = new_data[len(new_data) - len(valid) - 60:].values
inputs = inputs.reshape(-1,1)
inputs  = scaler.transform(inputs)

X_test = []
for i in range(60,inputs.shape[0]):
    X_test.append(inputs[i-60:i,0])
X_test = np.array(X_test)

X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))
closing_price = model.predict(X_test)
closing_price = scaler.inverse_transform(closing_price)
###Result
rms=np.sqrt(np.mean(np.power((valid-closing_price),2)))
rms

#for plotting
train = new_data[:987]
valid = new_data[987:]
valid['Predictions'] = closing_price
plt.plot(train['Close'])
plt.plot(valid[['Close','Predictions']])

###Conclusion
Wow ! the results are really suprising using only 1 epoches ! LSTM showed us that he is the MVP Algorithm so far when it comes to timeseries data.
The Memory and fact that lstm is taking that in consideration when training the model showed us its importance.
#General Conclusion
Working with time series datasets is not that easy, since most of machine learning algorithms are not smart to detect the overfit on the Date column during the model training.

Thus, LSTM is a good choice to be used especially when dealing with this kind of problems.
